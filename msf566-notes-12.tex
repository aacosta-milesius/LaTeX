\section{Multivariate Volatility Models}
\paragraph{Primary Text Reading.} \citeA[chap. 10]{tsay2005aft}\index{Tsay, Ruey}
and \citeA{lanne2007mgof}

Correlations between asset returns are important in many financial applications. In recent years, multivariate volatility models have been used to describe the time-varying feature of the correlations. However, the curse of dimensionality quickly becomes an issue as the number of correlations is $k(k -1)/2$ for $k$ assets \cite{tsay2006mvm}.

Let $\mathbf{r}_t = (r_{1t}, \ldots , r_{kt})$ be a vector of returns (or log returns) of $k$ assets at time index $t$. Let $\mathbf{F}_{t-1}$ be the sigma field generated by the past information at time index $t-1$. We partition the return $\mathbf{r}_t$ as
\begin{equation}
\mathbf{r}_t = \mu_t + e_t,
\end{equation}
where $\mu_t = E(r_t | \mathbf{F}_{t-1} )$ is the conditional mean of the return given $\mathbf{F}_{t-1}$ and $e_t$ is the innovation (\emph{or noise term}) satisfying $e_t = \sigma^{1/2}_t \epsilon_t$ such that 
\[
\text{Cov}(e_t | \mathbf{F}_{t-1} ) = \text{Cov}(r_t | \mathbf{F}_{t-1} ) = \Sigma_t,
\]
where $t = (\epsilon_{1t}, \ldots, \epsilon_{kt})'$ is a sequence of independently and identically distributed random vectors with mean zero and identity covariance matrix, and $\mathbf{\Sigma}^{1/2}_t$ is the symmetric square-root matrix of a positive-definite covariance matrix $\mathbf{\Sigma}_t$, that is, $\mathbf{\Sigma}^{1/2}_t \mathbf{\Sigma}^{1/2}_t = \mathbf{\Sigma}_t$. In the literature, $\mathbf{\Sigma}_t$ is often referred to as the volatility matrix. Volatility modeling is concerned with studying the evolution of the volatility matrix  over time. For asset returns, behavior of the conditional mean $\mu_t$ is relatively simple. 
In most cases, $\mu_t$ is simply a constant. In some cases, it may assume a simple vector autoregressive model. The volatility matrix $\mathbf{\Sigma}_t$, on the other hand, is much harder to model, and most GARCH studies in the literature focus entirely on modeling $\mathbf{\Sigma}_t$.

The conditional covariance matrix $\mathbf{\Sigma}_t$ can be written as 
\begin{equation}
\mathbf{\Sigma}_t = \mathbf{D}_t \mathbf{R}_t \mathbf{D}_t
\label{eq:cond-covar-matrix}
\end{equation}
where $\mathbf{D}_t$ is a diagonal matrix consisting of the conditional standard deviations of the returns, $\mathbf{D}_t = diag\{\sqrt{\mathbf{\Sigma}_{11},t}, \ldots, \sqrt{\mathbf{\Sigma}_{11},t}\}$ with $\mathbf{\Sigma}_{ij,t}$ being the $(i, j)$th element 
of $\mathbf{\Sigma}$, and $\mathbf{R}_t$ is the correlation matrix. 

\subsection{Vector Volatility Models} 
Many multivariate volatility models are available in the literature. In this section, we briefly review some of those models that are relevant to the proposed model. We shall focus on the simple models of order $(1, 1)$ in our discussion because such models are typically used in applications and the generalization to higher-order models is straightforward. In what follows, let $a_{ij}$ denote the $(i, j )$th element of the matrix $\mathbf{A}$ and $u_{it}$ be the $i$th element of the vector $\mathbf{u}_t$.

\subsection{Exponentially Weighted Estimates}
Given the innovations $\mathbf{F}_{t-1}=\{\mathbf{a}_1, \ldots,\mathbf{a}_{t-1}\}$, the unconditional covariance matrix of the innovation can be estimated by
\[
\widehat{\mathbf{\Sigma}}=\frac{1}{t-1} \sum^{t-1}_{j=1}\mathbf{a}_j\mathbf{a}'_j,
\]
where it is understood that the mean of $\mathbf{a}_j$ is zero. This estimate assigns equal weight $1/(t-1)$ to each term in the summation. To allow for a time-varying covariance matrix and to emphasize that recent innovations are more relevant, one can use the idea of exponential smoothing and estimate the covariance matrix of $\mathbf{a}_t$ by
\begin{equation}
\widehat{\mathbf{\Sigma}}_t=\frac{1-\lambda}{1-\lambda^{t-1}} \sum^{t-1}_{j=1}\lambda^{j-1} \mathbf{a}_{t-j}\mathbf{a}'_{t-j},
\label{eq:expon-wt-est}
\end{equation}
where $0< \lambda< 1$ and the weights $(1-\lambda)\lambda^{j-1}/ (1-\lambda^{t-1})$ sum to one. For a sufficiently large $t$ such that $\lambda^{t-1} \approx 0$, the prior equation can be rewritten as
\[
\widehat{\mathbf{\Sigma}}=(1-\lambda)\mathbf{a}_{t-1}\mathbf{a}'_{t-1} + \lambda \widehat{\mathbf{\Sigma}}_{t-1}.
\]
The covariance estimate in \eqref{eq:expon-wt-est} is referred to as the \emph{exponentially weighted moving average} (EWMA) estimate of the covariance matrix.

\subsection{Multivariate GARCH Models}
Just as we have been able to specify a \emph{univariate} GARCH model previously in Section~\ref{garch-spec}, we can examine some \emph{multivariate} GARCH models. Likewise, regression methods exist for either kind of model. Two nonparametric tests for investigating multivariate regression functions can be found in \citeA{RePEc:eee:econom:v:147:y:2008:i:1:p:151-162}.

\subsubsection{VEC Model}\index{VEC model} For a symmetric $n \times n$ matrix $\mathbf{A}$, let $\text{vech}(\mathbf{A})$ be the half-stacking vector of $\mathbf{A}$, that is, $\text{vech}(\mathbf{A})$ is a $n(n +1)/2 \times 1$ vector obtained by stacking the lower triangular portion of the matrix $\mathbf{A}$. Let $h_t = \text{vech}(\mathbf{\Sigma}_t)$ and $\eta_t = \text{vech}(e_t e'_t)$. Using the idea of exponential smoothing, the VEC model
\begin{equation}
h_t = c + \mathbf{A}_{\eta_t-1} + \mathbf{B}_{ht-1}
\label{eq:vec-model}
\end{equation}
where $c$ is a $k(k + 1)/2$ dimensional vector, and $\mathbf{A}$ and $\mathbf{B}$ are $k(k + 1)/2 \times k(k + 1)/2$ matrices. This model contains several weaknesses. First, the model contains $k(k + 1)[k(k + 1) + 1]/2$ parameters, which is large even for a small $k$. For instance, if 
$k = 3$, then the model contains 78 parameters, making it hard to apply in practice. To overcome this difficulty, it is suggested that both $\mathbf{A}$ and $\mathbf{B}$ matrices of \eqref{eq:vec-model} are constrained to be diagonal. The second weakness of the model is that the resulting volatility matrix $\mathbf{\Sigma}_t$ may not be positive definite.

\subsubsection{BEKK Model}\index{BEKK model}
\margincomment{BEKK is named for Baba, Engle, Kraft, and Kroner}
A simple BEKK model assumes the form
\begin{equation}
\mathbf{\Sigma}_t = \mathbf{C}'\mathbf{C} + \mathbf{A}' e_{t-1} e'_{t-1} \mathbf{A}+\mathbf{B}' \mathbf{\Sigma}_{t-1} \mathbf{B}
\label{eq:bekk}
\end{equation}
where $\mathbf{C}$, $\mathbf{A}$, and $\mathbf{B}$ are $k \times k$ matrices but $\mathbf{C}$ is upper triangular. An advantage of the BEKK model is that $\mathbf{\Sigma}_t$ is positive definite if the diagonal elements of $\mathbf{C}$ is positive. On the other hand, the model contains many parameters that do not represent directly the impact of $e_{t-1}$ or $\mathbf{\Sigma}_{t-1}$ on the elements of $\mathbf{\Sigma}_t$. In other words, it is 
hard to interpret the parameters of a BEKK model. Limited experience also shows that many parameter estimates of the BEKK model in \eqref{eq:bekk} are statistically insignificant, implying that the model is overparameterized. 

Using the standardization of \eqref{eq:cond-covar-matrix}, one can divide the multivariate volatility modeling into two steps. The first step is to specify models for elements of the $\mathbf{D}_t$ matrix, and the second step is to model the correlation matrix $\mathbf{R}_t$ . Two such approaches have been proposed in the literature. In both cases, the elements $\sigma_{ii,t}$ are assumed to follow a univariate GARCH model. In other words, $\sigma_{ii,t}$ are based entirely on the $i$-th return series. 

The individual volatility $\sigma_{ii,t}$ can assume any univariate GARCH models, and the correlation matrix $\mathbf{R}_t$ of \eqref{eq:cond-covar-matrix} follows the model
\begin{equation}
\mathbf{R}_t = (1-\lambda_1 -\lambda_2)\mathbf{R} + \lambda_1 \Psi_{t-1} + \lambda_2 \mathbf{R}_{t-1}
\label{eq:dyn-corr}
\end{equation}
where $\lambda_1$ and $\lambda_2$ are non-negative parameters satisfying $0 \le \lambda_1 + \lambda_2 < 1$, $\mathbf{R}$ is a $k \times k$ positive definite parameter matrix with $\mathbf{R}_{ii} = 1$ and $\Psi_{t-1}$ is the $k \times k$ correlation matrix of some recent asset returns. For instance, if the most recent $m$ returns are 
used to define $\Psi_t-1$, then the $(i, j )$th element of $\Psi_{t-1}$ is given by
\[
\Psi_{ij,t-1} = \frac{\sum^m_{v=1}u_{i,t-v}u_{j,t-v}}{\sqrt{\sum^m_{v=1}u^2_{i,t-v} u^2_{i,t-v}}}  
\]
where $u_{it} = e_{it} / \sqrt{\sigma{ii,t}}$ . If $m > k$, then $\Psi_t-1$ is positive definite almost surely. This in turn implies that $\mathbf{R}_t$ is positive definite almost surely. We refer to this model as a $DCC_T(m)$ model. In practice, one can use the sample correlation matrix of the data to estimate $\mathbf{R}$ in order to simplify the calculation. Indeed, this is the approach we shall take.

From the definition, the use of $DCC_T(m)$ model involves two steps. In the first step, univariate GARCH models are built for each return series. At step 2, the correlation matrix $\mathbf{R}_t$ \eqref{eq:cond-covar-matrix} is estimated across all return series via the maximum likelihood method. An advantage of the $DCC_T (m)$ model is that the resulting correlation matrices are positive definite almost surely. In addition, the model is parsimonious in parameterization because the evolution of correlation matrices is governed by two parameters. On the other hand, strong limitation is imposed on the time evolution of the correlation matrices. In addition, it is hard to interpret the results of the two-step estimation. For instance, it is not clear what is the joint distribution of the innovation e of the return series. 

We can demonstrate a concept using the R environment. In this case, we can use \texttt{mgarchBEKK}, although the package is in development, and only has full support in the Unix environment.
\rpkg{mgarchBEKK}

\subsection{Reparameterization}
Because $\mathbf{\Sigma}_t$ is a symmetrical matrix, we have two options for \emph{reparameterization}. These methods allow us to alter the parameter correlation in order to make a better-fitting model.

\subsubsection{Correlation Reparameterization}
Our first method allows us to reparameterize using the conditional coffecients and variances of $\mathbf{a}_t$, and we can write $\mathbf{\Sigma}_t$ as
\[
\mathbf{\Sigma}_t \equiv [\sigma_{ij,t}] = \mathbf{D}_t \mathbf{\rho}_t \mathbf{D}_t,
\]
where $\mathbf{\rho}_t$ is the conditional correlation matrix of $\mathbf{a}_t$ and $\mathbf{D}_t$ is a $k \times k$ diagonal matrix consisting of the conditional standard deviations of elements of $\mathbf{a}_t$, which is,
\[
\mathbf{D}_t = \text{diag}\Big( \{\sqrt{\sigma_{11,t}}, \ldots, \sqrt{\sigma_{kk,t}} \} \Big).
\]

However, as the matrix $\mathbf{\Sigma}_t$ gets large, it requires parameter constraints, and the function becomes increasingly complicated.

\subsubsection{Cholesky Decomposition}\index{Cholesky decomposition}
Because $\mathbf{\Sigma}_t$ is positive definite, there is a lower triangle matrix $\mathbf{L}_t$ with unit diagonal elements and a diagonal matrix $\mathbf{G}_t$ with positive diagonal elements such that
\begin{equation}
\mathbf{\Sigma}_t = \mathbf{L}_t \mathbf{G}\mathbf{L}'_t,
\label{eq:cholesky-decomp}
\end{equation}
which is the \emph{Cholesky decomposition} of $\mathbf{\Sigma}_t$.

% Might be good to include the matrices from Tsay, pp.455-456.

% \subsection{GARCH Models for Bivariate Returns}
%\paragraph{Constant-Correlation Models.}

\subsection{Higher Dimension Volatility}

In some situations, matrix multiplication in vector models such as these may rely upon a \emph{Kronecker product}, which is $(\mathbf{A}_i \otimes \mathbf{A}_i)'$.\index{Kronecker product}

If $\mathbf{A}$ is an $m$-by-$n$ matrix and $\mathbf{B}$ is a $p \times q$ matrix, then the Kronecker product $\mathbf{A}\otimes\mathbf{B}$ is the $mp \times nq$ block matrix
\[
\mathbf{A}\otimes\mathbf{B} = \begin{bmatrix} a_{11} B & \cdots & a_{1n}B \\ \vdots & \ddots & \vdots \\ a_{m1} B & \cdots & a_{mn} B \end{bmatrix}. 
\]
More explicitly, we have
\[
\mathbf{A}\otimes\mathbf{B} = \begin{bmatrix}    a_{11} b_{11} & a_{11} b_{12} & \cdots & a_{11} b_{1q} &                     \cdots & \cdots & a_{1n} b_{11} & a_{1n} b_{12} & \cdots & a_{1n} b_{1q} \\    a_{11} b_{21} & a_{11} b_{22} & \cdots & a_{11} b_{2q} &                     \cdots & \cdots & a_{1n} b_{21} & a_{1n} b_{22} & \cdots & a_{1n} b_{2q} \\    \vdots & \vdots & \ddots & \vdots & & & \vdots & \vdots & \ddots & \vdots \\    a_{11} b_{p1} & a_{11} b_{p2} & \cdots & a_{11} b_{pq} &                     \cdots & \cdots & a_{1n} b_{p1} & a_{1n} b_{p2} & \cdots & a_{1n} b_{pq} \\    \vdots & \vdots & & \vdots & \ddots & & \vdots & \vdots & & \vdots \\    \vdots & \vdots & & \vdots & & \ddots & \vdots & \vdots & & \vdots \\    a_{m1} b_{11} & a_{m1} b_{12} & \cdots & a_{m1} b_{1q} &                     \cdots & \cdots & a_{mn} b_{11} & a_{mn} b_{12} & \cdots & a_{mn} b_{1q} \\    a_{m1} b_{21} & a_{m1} b_{22} & \cdots & a_{m1} b_{2q} &                     \cdots & \cdots & a_{mn} b_{21} & a_{mn} b_{22} & \cdots & a_{mn} b_{2q} \\    \vdots & \vdots & \ddots & \vdots & & & \vdots & \vdots & \ddots & \vdots \\    a_{m1} b_{p1} & a_{m1} b_{p2} & \cdots & a_{m1} b_{pq} &                     \cdots & \cdots & a_{mn} b_{p1} & a_{mn} b_{p2} & \cdots & a_{mn} b_{pq}  \end{bmatrix}. 
\]

To demonstrate the difference between matrix multiplication introduced in Section~\ref{linear-algebra} and a Kronecker product, note the following,
\[
\begin{bmatrix}
	1 & 2 \\
	3 & 4 \\
\end{bmatrix}
\otimes
\begin{bmatrix}
      0 & 5 \\
      6 & 7 \\
\end{bmatrix} =
\begin{bmatrix}
	1\cdot 0 & 1\cdot 5 & 2\cdot 0 & 2\cdot 5 \\
	1\cdot 6 & 1\cdot 7 & 2\cdot 6 & 2\cdot 7 \\      
	3\cdot 0 & 3\cdot 5 & 4\cdot 0 & 4\cdot 5 \\      
	3\cdot 6 & 3\cdot 7 & 4\cdot 6 & 4\cdot 7 \\    
\end{bmatrix}  =
\begin{bmatrix}
	0 & 5 & 0 & 10 \\      
	6 & 7 & 12 & 14 \\     
	0 & 15 & 0 & 20 \\     
	18 & 21 & 24 & 28
\end{bmatrix}
\]

which, when we use variables instead, is,
\[
\begin{bmatrix} a_{11} & a_{12} \\ a_{21} & a_{22} \\ a_{31} & a_{32} \end{bmatrix} \otimes \begin{bmatrix} b_{11} & b_{12} & b_{13} \\ b_{21} & b_{22} & b_{23} \end{bmatrix} = \begin{bmatrix} a_{11} b_{11} & a_{11} b_{12} & a_{11} b_{13} & a_{12} b_{11} & a_{12} b_{12} & a_{12} b_{13} \\ a_{11} b_{21} & a_{11} b_{22} & a_{11} b_{23} & a_{12} b_{21} & a_{12} b_{22} & a_{12} b_{23} \\ a_{21} b_{11} & a_{21} b_{12} & a_{21} b_{13} & a_{22} b_{11} & a_{22} b_{12} & a_{22} b_{13} \\ a_{21} b_{21} & a_{21} b_{22} & a_{21} b_{23} & a_{22} b_{21} & a_{22} b_{22} & a_{22} b_{23} \\ a_{31} b_{11} & a_{31} b_{12} & a_{31} b_{13} & a_{32} b_{11} & a_{32} b_{12} & a_{32} b_{13} \\ a_{31} b_{21} & a_{31} b_{22} & a_{31} b_{23} & a_{32} b_{21} & a_{32} b_{22} & a_{32} b_{23} \end{bmatrix}
\]

%\subsection{Factor Volatility Models}

\subsection{Eigenvalue Example Using Fixed Income Scenarios}
To demonstrate using principal component analysis\index{principal component analysis} and eigenvalue decomposition\index{eigenvalue decomposition} in Monte Carlo simulation of a yield curve comprising several fixed income instruments, we turn to an example in \citeA{marrison2002frm}. To begin, consider a U.S. government yield curve. We have statistics in the following matrices: standard deviation $\mathbf{S}$, correlation $\mathbf{R}$, and covariance $\mathbf{C}$ for absolute changes in the 3-month, 1-year, 5-year, and 20-year interest rates. Using these matrices, we want to create a matrix $\mathbf{B}$ that will assist us in creating random scenarios.

\begin{eqnarray*}
\mathbf{S} &=&
\begin{bmatrix}
	0.051 & 0.052 & 0.061 & 0.054
\end{bmatrix} \\
\\
\mathbf{R} &=&
\begin{bmatrix}
	1 & 0.61 & 0.42 & 0.31 \\
	0.61 & 1 & 0.83 & 0.67 \\
	0.42 & 0.83 & 1 & 0.88 \\
	0.31 & 0.67 & 0.88 & 1
\end{bmatrix} \\
\\
\mathbf{C} &=& \text{diag}(S) \times R \times \text{diag}(S)
%\mathbf{C} &=& \mathbf{S}^T \mathbf{R}\mathbf{S} \\
\\
&=&
\begin{bmatrix}
	.0026 & .0016 & .0013 & .0008 \\
	.0016 & .0027 & .0026 & .0019 \\
	.0013 & .0026 & .0038 & .0029 \\
	.0008 & .0019 & .0029 & .0029
\end{bmatrix}.
\end{eqnarray*}
We obtain the eigenvalue decomposition of $\mathbf{C}$ in MATLAB or Octave using,
\ecaption{Eigenvalue Decomposition in MATLAB}
\begin{verbatim}
    lambda_sqrt = sqrt(eig(C));
    eigv_decom=diag(lambda_sqrt)
\end{verbatim}

\[
\mathbf{\Lambda}^{1/2}=
\begin{bmatrix}
	0.017804 & 0 & 0 &  0 \\
	0 & 0.024739 & 0 &  0 \\
	0 & 0 & 0.046721 & 0 \\
	0 & 0 & 0 & 0.094277 \\
\end{bmatrix}
\]
Eigenvectors have special properties that are useful for speeding up Monte Carlo simulations. Each eigenvector defines a market movement that is by definition independent of the other movements, because $\mathbf{I}=\mathbf{E}^T\mathbf{E}$. Also, $\mathbf{C}=\mathbf{E}^T \text{diag}(\mathbf{\Lambda}) \mathbf{E}$. Our $\mathbf{E}$ matrix is,
\[
\mathbf{E}=
\begin{bmatrix}
	\phantom{-}0.097 & -0.480 & \phantom{-}0.724 & -0.486 \\
	\phantom{-}0.407 & -0.694 & -0.124 & \phantom{-}0.581 \\
	-0.847 & -0.195 & \phantom{-}0.264 & \phantom{-}0.417 \\
	\phantom{-}0.327 & \phantom{-}0.500 & \phantom{-}0.625 & \phantom{-}0.502 \\
\end{bmatrix}
\]
How do we know this is an $\mathbf{E}$ matrix? Because \texttt{abs(round(transpose(E)*E))} makes a $4 \times 4$ identity matrix. Finally, we compute $\mathbf{B}=\mathbf{\Lambda}^{1/2} \mathbf{E}$,
\[
\mathbf{B}=
\begin{bmatrix}
	\phantom{-}0.0017269 & -0.0085457 & \phantom{-}0.0128898 & -0.0086525 \\
	\phantom{-}0.0100687 & -0.0171688 & -0.0030676 & \phantom{-}0.0143733 \\
	-0.0395723 & -0.0091105 & \phantom{-}0.0123342 & \phantom{-}0.0194825 \\
	\phantom{-}0.0308287 & \phantom{-}0.0471387 & \phantom{-}0.0589233 & \phantom{-}0.0473272
\end{bmatrix}
\]
We have four principal components, named \emph{wiggle}, \emph{flex}, \emph{twist}, and \emph{shift}. We can use the matrix $\mathbf{B}$ to create random scenarios,
\[
\begin{bmatrix}
\delta r_{3mo} & \delta r_{1yr} & \delta r_{5yr} & \delta r_{20yr}
\end{bmatrix}
=
\begin{bmatrix}
z_1 & z_2 & z_3 & z_4
\end{bmatrix}
\times \mathbf{B}.
\]
where $z_n$ is a random number generated for factor $n$. Each row of $\mathbf{B}$ describes its sensitivity to changes in the factor and the rate point,
\begin{eqnarray*}
wiggle &=& z_1 
\begin{bmatrix}
\phantom{-}0.0017269 & -0.0085457 & \phantom{-}0.0128898 & -0.0086525
\end{bmatrix}\\
flex &=& z_2
\begin{bmatrix}
\phantom{-}0.0100687 & -0.0171688 & -0.0030676 & \phantom{-}0.0143733
\end{bmatrix} \\
twist &=& z_3
\begin{bmatrix}
-0.0395723 & -0.0091105 & \phantom{-}0.0123342 & \phantom{-}0.0194825
\end{bmatrix} \\
shift &=& z_4
\begin{bmatrix}
\underbrace{\phantom{-}0.0308287}_{3mo} & \underbrace{\phantom{-}0.0471387}_{1yr} & \underbrace{\phantom{-}0.0589233}_{5yr} & \underbrace{\phantom{-}0.0473272}_{20yr}
\end{bmatrix}.
\end{eqnarray*}
We can see that for any given change in $z_n$, the \emph{shift} has the strongest effect, and \emph{wiggle} is the weakest.

For instance, if $z_4$ increases by 1, all of the rate points shift up by the magnitude of the bottom row of $\mathbf{B}$, which means the 3-month rate increases by 3 bps, the 1-year by 5 bps, the 5-year by 6 bps, and the 20-year by 5 bps. Notice that a positive change of 1 to $z_3$ would cause \emph{twist} in the 3-month rate to decrease 4 bps, and the 1-year to decrease 1 bp, but the 5- and 20-year rates increase 1 and 2 bps, respectively.

Since \emph{wiggle} and \emph{flex} have the least amount of influence, if we wanted to optimize our Monte Carlo simulation by reducing the number of random variables, we could hold $z_1$ and $z_2$ constant, and let $z_3$ and $z_4$ be random.

Imagine a process where we capture thousands of iterations of the following simulations. We will hold \emph{wiggle} and \emph{flex} constant, and will apply random shocks to \emph{twist} and \emph{shift}. We set $m$ to the magnitude of the random shock, and use \texttt{normrnd} to generate a normally-distributed random number with $\mu=0$ and $\sigma=1$.
\ecaption{Yield Curve Shocks in MATLAB}
\begin{verbatim}
    m=2
    round([1 1 m*normrnd(0,1) m*normrnd(0,1)]*(100*B))
\end{verbatim}
This gives us a row vector of basis point shocks for each rate point. For example, in one trial, the rate shocks $\delta r$ were obtained.
\[
\underbrace{-3 \text{ bps}}_{3mo} \quad \underbrace{-2 \text{ bps}}_{1yr} \quad \underbrace{6 \text{ bps}}_{5yr} \quad \underbrace{8 \text{ bps}}_{20yr}.
\]
